{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the standard DQN algorithm proposed by https://www.nature.com/articles/nature14236 and we understand the significance of each of the factors by varying them and analysing the performance of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - 'CartPole - v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain about state space, action space, reward (discount?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1- (+Q + E + T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from config1 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "no_siblings = 3\n",
    "sibling_scores = []\n",
    "sibling_lives = np.zeros(no_siblings)\n",
    "\n",
    "\n",
    "def dqn(n_episodes=20000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                 # list containing scores from each episode\n",
    "    scores_window_printing = deque(maxlen=10) # For printing in the graph\n",
    "    scores_window= deque(maxlen=100)  # last 100 scores for checking if the avg is more than 195\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_window_printing.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))        \n",
    "        if i_episode % 100 == 0: \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return [np.array(scores),i_episode-100]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "for i in range(no_siblings):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    [temp_scores,sibling_lives[i]] = dqn()\n",
    "    sibling_scores.append(temp_scores)\n",
    "    plt.plot(np.arange(len(temp_scores)), temp_scores)\n",
    "    \n",
    "        \n",
    "    \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 26.3  30.3  21.5  35.1  36.   52.7  37.2  65.7  81.3  86.9  75.9  86.3\n",
      " 126.8  96.  131.6  97.3  77.   62.3  73.6  90.3 114.1 137.5 114.5 120.2\n",
      "  96.4  11.7  20.3  10.   11.6  33.9  76.3 193.7 200.  200.  200.  200.\n",
      " 200.  200.  200.  200. ]\n",
      "-----\n",
      "307\n"
     ]
    }
   ],
   "source": [
    "print(temp_scores)\n",
    "print('-----')\n",
    "print(temp_life)\n",
    "\n",
    "sibling_scores.append(temp_scores)\n",
    "sibling_lives[i] = temp_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2- (+Q + E - T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config2 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores2 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores2)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores2\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores2 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores2[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores2 = np.divide(scores2,avg_iter)\n",
    "    \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores2)), scores2)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3- (+Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from config3 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores3 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores3)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores3\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores1 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores3[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores3 = np.divide(scores3,avg_iter)\n",
    "    \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores3)), scores3)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4- (+Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config4 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores4 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores4)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores4\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores1 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores4[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores4 = np.divide(scores4,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores4)), scores4)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5- (-Q +E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Networks Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config5 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores5 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores5)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores5\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores5 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores5[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores5 = np.divide(scores5,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores5)), scores5)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: (-Q +E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config6 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "n_players = 10\n",
    "results = np.zeros([])\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores6 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores6)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores6\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores6 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores6[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores6 = np.divide(scores6,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores6)), scores6)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 7: (-Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config7 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores7 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores7)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores7\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores7 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores7[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores7 = np.divide(scores7,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores7)), scores7)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 8: (-Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config8 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores8 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores8)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores8\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores8 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores8[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores8 = np.divide(scores8,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores8)), scores8)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='case 1')\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='g', label='case 2')\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='r', label='case 3')\n",
    "plt.plot(np.arange(len(scores4)), scores4, color='c', label='case 4')\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='case 5')\n",
    "plt.plot(np.arange(len(scores6)), scores6, color='y', label='case 6')\n",
    "plt.plot(np.arange(len(scores7)), scores7, color='k', label='case 7')\n",
    "plt.plot(np.arange(len(scores8)), scores8, color='w', label='case 8')\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Target Network on')\n",
    "\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='Target Network off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='EXP Replay on')\n",
    "\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='m', label='EXP Replay off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Truncation on')\n",
    "\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='m', label='Truncation off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
