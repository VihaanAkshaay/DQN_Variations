{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the standard DQN algorithm proposed by https://www.nature.com/articles/nature14236 and we understand the significance of each of the factors by varying them and analysing the performance of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - 'CartPole - v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain about state space, action space, reward (discount?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1- (+Q + E + T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 17.36\n",
      "Episode 200\tAverage Score: 12.33\n",
      "Episode 300\tAverage Score: 10.70\n",
      "Episode 400\tAverage Score: 10.04\n",
      "Episode 500\tAverage Score: 9.990\n",
      "Episode 600\tAverage Score: 10.29\n",
      "Episode 700\tAverage Score: 10.74\n",
      "Episode 800\tAverage Score: 11.84\n",
      "Episode 900\tAverage Score: 18.98\n",
      "Episode 1000\tAverage Score: 28.06\n",
      "Episode 1100\tAverage Score: 32.56\n",
      "Episode 1200\tAverage Score: 41.91\n",
      "Episode 1300\tAverage Score: 121.20\n",
      "Episode 1400\tAverage Score: 137.95\n",
      "Episode 1500\tAverage Score: 179.20\n",
      "Episode 1544\tAverage Score: 195.43\n",
      "Environment solved in 1444 episodes!\tAverage Score: 195.43\n",
      "Episode 100\tAverage Score: 16.47\n",
      "Episode 200\tAverage Score: 12.14\n",
      "Episode 300\tAverage Score: 11.21\n",
      "Episode 400\tAverage Score: 10.01\n",
      "Episode 500\tAverage Score: 10.24\n",
      "Episode 600\tAverage Score: 11.20\n",
      "Episode 700\tAverage Score: 20.37\n",
      "Episode 800\tAverage Score: 48.96\n",
      "Episode 900\tAverage Score: 79.58\n",
      "Episode 1000\tAverage Score: 120.22\n",
      "Episode 1100\tAverage Score: 123.88\n",
      "Episode 1200\tAverage Score: 152.90\n",
      "Episode 1300\tAverage Score: 188.19\n",
      "Episode 1378\tAverage Score: 195.11\n",
      "Environment solved in 1278 episodes!\tAverage Score: 195.11\n",
      "Episode 100\tAverage Score: 17.57\n",
      "Episode 200\tAverage Score: 11.85\n",
      "Episode 300\tAverage Score: 10.91\n",
      "Episode 400\tAverage Score: 10.88\n",
      "Episode 500\tAverage Score: 12.34\n",
      "Episode 600\tAverage Score: 21.62\n",
      "Episode 700\tAverage Score: 33.48\n",
      "Episode 800\tAverage Score: 87.18\n",
      "Episode 900\tAverage Score: 152.37\n",
      "Episode 1000\tAverage Score: 184.53\n",
      "Episode 1100\tAverage Score: 192.52\n",
      "Episode 1200\tAverage Score: 122.16\n",
      "Episode 1300\tAverage Score: 115.07\n",
      "Episode 1398\tAverage Score: 195.45\n",
      "Environment solved in 1298 episodes!\tAverage Score: 195.45\n",
      "Episode 100\tAverage Score: 15.77\n",
      "Episode 200\tAverage Score: 12.11\n",
      "Episode 300\tAverage Score: 11.05\n",
      "Episode 400\tAverage Score: 11.33\n",
      "Episode 500\tAverage Score: 20.21\n",
      "Episode 600\tAverage Score: 91.04\n",
      "Episode 700\tAverage Score: 178.44\n",
      "Episode 800\tAverage Score: 179.90\n",
      "Episode 900\tAverage Score: 178.99\n",
      "Episode 1000\tAverage Score: 150.34\n",
      "Episode 1100\tAverage Score: 156.36\n",
      "Episode 1200\tAverage Score: 141.94\n",
      "Episode 1300\tAverage Score: 61.826\n",
      "Episode 1400\tAverage Score: 111.42\n",
      "Episode 1500\tAverage Score: 182.24\n",
      "Episode 1600\tAverage Score: 124.87\n",
      "Episode 1700\tAverage Score: 40.943\n",
      "Episode 1800\tAverage Score: 123.36\n",
      "Episode 1900\tAverage Score: 138.71\n",
      "Episode 2000\tAverage Score: 103.11\n",
      "Episode 100\tAverage Score: 16.52\n",
      "Episode 200\tAverage Score: 11.75\n",
      "Episode 300\tAverage Score: 11.62\n",
      "Episode 400\tAverage Score: 11.36\n",
      "Episode 500\tAverage Score: 14.22\n",
      "Episode 600\tAverage Score: 20.48\n",
      "Episode 700\tAverage Score: 81.91\n",
      "Episode 800\tAverage Score: 160.23\n",
      "Episode 900\tAverage Score: 190.01\n",
      "Episode 1000\tAverage Score: 190.62\n",
      "Episode 1100\tAverage Score: 189.33\n",
      "Episode 1200\tAverage Score: 172.71\n",
      "Episode 1300\tAverage Score: 118.42\n",
      "Episode 1400\tAverage Score: 110.00\n",
      "Episode 1500\tAverage Score: 114.13\n",
      "Episode 1600\tAverage Score: 188.23\n",
      "Episode 1615\tAverage Score: 196.10\n",
      "Environment solved in 1515 episodes!\tAverage Score: 196.10\n",
      "Episode 100\tAverage Score: 17.69\n",
      "Episode 200\tAverage Score: 12.61\n",
      "Episode 300\tAverage Score: 11.20\n",
      "Episode 400\tAverage Score: 10.63\n",
      "Episode 500\tAverage Score: 12.84\n",
      "Episode 600\tAverage Score: 21.28\n",
      "Episode 700\tAverage Score: 24.86\n",
      "Episode 800\tAverage Score: 37.08\n",
      "Episode 900\tAverage Score: 49.82\n",
      "Episode 1000\tAverage Score: 102.40\n",
      "Episode 1100\tAverage Score: 186.58\n",
      "Episode 1114\tAverage Score: 195.66\n",
      "Environment solved in 1014 episodes!\tAverage Score: 195.66\n",
      "Episode 100\tAverage Score: 16.97\n",
      "Episode 200\tAverage Score: 12.08\n",
      "Episode 300\tAverage Score: 11.17\n",
      "Episode 400\tAverage Score: 11.48\n",
      "Episode 500\tAverage Score: 15.23\n",
      "Episode 600\tAverage Score: 21.93\n",
      "Episode 700\tAverage Score: 35.26\n",
      "Episode 800\tAverage Score: 106.40\n",
      "Episode 900\tAverage Score: 151.86\n",
      "Episode 1000\tAverage Score: 119.06\n",
      "Episode 1100\tAverage Score: 116.47\n",
      "Episode 1200\tAverage Score: 145.77\n",
      "Episode 1300\tAverage Score: 153.32\n",
      "Episode 1400\tAverage Score: 154.88\n",
      "Episode 1500\tAverage Score: 185.84\n",
      "Episode 1600\tAverage Score: 181.01\n",
      "Episode 1700\tAverage Score: 113.26\n",
      "Episode 1800\tAverage Score: 117.16\n",
      "Episode 1900\tAverage Score: 100.84\n",
      "Episode 2000\tAverage Score: 115.38\n",
      "Episode 100\tAverage Score: 16.55\n",
      "Episode 200\tAverage Score: 11.78\n",
      "Episode 300\tAverage Score: 11.14\n",
      "Episode 400\tAverage Score: 11.01\n",
      "Episode 500\tAverage Score: 19.28\n",
      "Episode 600\tAverage Score: 103.52\n",
      "Episode 700\tAverage Score: 155.04\n",
      "Episode 800\tAverage Score: 187.12\n",
      "Episode 900\tAverage Score: 157.69\n",
      "Episode 1000\tAverage Score: 175.94\n",
      "Episode 1100\tAverage Score: 182.36\n",
      "Episode 1200\tAverage Score: 178.05\n",
      "Episode 1300\tAverage Score: 140.30\n",
      "Episode 1400\tAverage Score: 121.54\n",
      "Episode 1500\tAverage Score: 105.34\n",
      "Episode 1600\tAverage Score: 111.18\n",
      "Episode 1700\tAverage Score: 120.71\n",
      "Episode 1800\tAverage Score: 111.97\n",
      "Episode 1900\tAverage Score: 123.35\n",
      "Episode 2000\tAverage Score: 48.578\n",
      "Episode 100\tAverage Score: 18.53\n",
      "Episode 200\tAverage Score: 12.42\n",
      "Episode 300\tAverage Score: 11.19\n",
      "Episode 400\tAverage Score: 11.09\n",
      "Episode 500\tAverage Score: 12.90\n",
      "Episode 600\tAverage Score: 29.58\n",
      "Episode 700\tAverage Score: 126.07\n",
      "Episode 800\tAverage Score: 149.88\n",
      "Episode 900\tAverage Score: 155.53\n",
      "Episode 1000\tAverage Score: 153.48\n",
      "Episode 1100\tAverage Score: 167.50\n",
      "Episode 1200\tAverage Score: 145.89\n",
      "Episode 1300\tAverage Score: 137.35\n",
      "Episode 1400\tAverage Score: 121.67\n",
      "Episode 1500\tAverage Score: 94.587\n",
      "Episode 1600\tAverage Score: 128.78\n",
      "Episode 1700\tAverage Score: 157.11\n",
      "Episode 1800\tAverage Score: 180.23\n",
      "Episode 1878\tAverage Score: 195.24\n",
      "Environment solved in 1778 episodes!\tAverage Score: 195.24\n",
      "Episode 100\tAverage Score: 16.37\n",
      "Episode 200\tAverage Score: 12.08\n",
      "Episode 300\tAverage Score: 11.27\n",
      "Episode 400\tAverage Score: 11.70\n",
      "Episode 500\tAverage Score: 19.54\n",
      "Episode 600\tAverage Score: 24.84\n",
      "Episode 700\tAverage Score: 28.58\n",
      "Episode 800\tAverage Score: 43.32\n",
      "Episode 900\tAverage Score: 75.80\n",
      "Episode 1000\tAverage Score: 150.96\n",
      "Episode 1100\tAverage Score: 179.36\n",
      "Episode 1182\tAverage Score: 195.22\n",
      "Environment solved in 1082 episodes!\tAverage Score: 195.22\n",
      "Episode 100\tAverage Score: 17.49\n",
      "Episode 200\tAverage Score: 12.35\n",
      "Episode 300\tAverage Score: 10.73\n",
      "Episode 400\tAverage Score: 10.21\n",
      "Episode 500\tAverage Score: 10.57\n",
      "Episode 600\tAverage Score: 12.21\n",
      "Episode 700\tAverage Score: 15.24\n",
      "Episode 800\tAverage Score: 58.03\n",
      "Episode 900\tAverage Score: 132.73\n",
      "Episode 1000\tAverage Score: 178.65\n",
      "Episode 1081\tAverage Score: 195.06\n",
      "Environment solved in 981 episodes!\tAverage Score: 195.06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from config1 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return np.array(scores)\n",
    "\n",
    "scores1 = dqn()\n",
    "    \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores1)), scores1)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2- (+Q + E - T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 16.89\n",
      "Episode 200\tAverage Score: 12.48\n",
      "Episode 300\tAverage Score: 10.67\n",
      "Episode 400\tAverage Score: 10.42\n",
      "Episode 500\tAverage Score: 11.05\n",
      "Episode 600\tAverage Score: 12.24\n",
      "Episode 700\tAverage Score: 22.72\n",
      "Episode 800\tAverage Score: 37.78\n",
      "Episode 900\tAverage Score: 65.86\n",
      "Episode 1000\tAverage Score: 141.51\n",
      "Episode 1100\tAverage Score: 152.81\n",
      "Episode 1200\tAverage Score: 135.08\n",
      "Episode 1300\tAverage Score: 145.85\n",
      "Episode 1400\tAverage Score: 148.40\n",
      "Episode 1500\tAverage Score: 163.91\n",
      "Episode 1600\tAverage Score: 152.36\n",
      "Episode 1700\tAverage Score: 132.19\n",
      "Episode 1800\tAverage Score: 145.16\n",
      "Episode 1900\tAverage Score: 149.16\n",
      "Episode 2000\tAverage Score: 151.49\n",
      "Episode 100\tAverage Score: 17.25\n",
      "Episode 200\tAverage Score: 12.39\n",
      "Episode 300\tAverage Score: 11.01\n",
      "Episode 400\tAverage Score: 10.34\n",
      "Episode 500\tAverage Score: 10.09\n",
      "Episode 600\tAverage Score: 9.981\n",
      "Episode 700\tAverage Score: 10.17\n",
      "Episode 800\tAverage Score: 11.03\n",
      "Episode 900\tAverage Score: 11.37\n",
      "Episode 1000\tAverage Score: 13.48\n",
      "Episode 1100\tAverage Score: 69.70\n",
      "Episode 1200\tAverage Score: 75.77\n",
      "Episode 1300\tAverage Score: 133.83\n",
      "Episode 1400\tAverage Score: 131.87\n",
      "Episode 1500\tAverage Score: 136.48\n",
      "Episode 1600\tAverage Score: 126.94\n",
      "Episode 1700\tAverage Score: 134.30\n",
      "Episode 1800\tAverage Score: 150.12\n",
      "Episode 1900\tAverage Score: 163.59\n",
      "Episode 2000\tAverage Score: 163.72\n",
      "Episode 100\tAverage Score: 18.18\n",
      "Episode 200\tAverage Score: 11.83\n",
      "Episode 300\tAverage Score: 11.10\n",
      "Episode 400\tAverage Score: 10.44\n",
      "Episode 500\tAverage Score: 11.41\n",
      "Episode 600\tAverage Score: 21.05\n",
      "Episode 700\tAverage Score: 25.33\n",
      "Episode 800\tAverage Score: 34.20\n",
      "Episode 900\tAverage Score: 28.92\n",
      "Episode 1000\tAverage Score: 26.55\n",
      "Episode 1100\tAverage Score: 31.35\n",
      "Episode 1200\tAverage Score: 74.24\n",
      "Episode 1257\tAverage Score: 135.48"
     ]
    }
   ],
   "source": [
    "from config2 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores2 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores2)), scores2)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3- (+Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from config3 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores3 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores3)), scores3)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4- (+Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config4 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores4 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores4)), scores4)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5- (-Q +E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Networks Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config5 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores5 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores5)), scores5)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: (-Q +E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config6 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores6 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores6)), scores6)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 7: (-Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config7 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores7 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores7)), scores7)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 8: (-Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config8 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores8 = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores8)), scores8)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='case 1')\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='g', label='case 2')\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='r', label='case 3')\n",
    "plt.plot(np.arange(len(scores4)), scores4, color='c', label='case 4')\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='case 5')\n",
    "plt.plot(np.arange(len(scores6)), scores6, color='y', label='case 6')\n",
    "plt.plot(np.arange(len(scores7)), scores7, color='k', label='case 7')\n",
    "plt.plot(np.arange(len(scores8)), scores8, color='w', label='case 8')\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Target Network on')\n",
    "\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='Target Network off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='EXP Replay on')\n",
    "\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='m', label='EXP Replay off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Truncation on')\n",
    "\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='m', label='Truncation off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
