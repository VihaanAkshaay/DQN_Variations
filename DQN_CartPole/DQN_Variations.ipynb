{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the standard DQN algorithm proposed by https://www.nature.com/articles/nature14236 and we understand the significance of each of the factors by varying them and analysing the performance of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - 'CartPole - v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain about state space, action space, reward (discount?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (6,)\n",
      "Number of actions:  3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1- (+Q + E + T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -378.94\n",
      "Episode 200\tAverage Score: -184.69\n",
      "Episode 300\tAverage Score: -115.21\n",
      "Episode 400\tAverage Score: -106.69\n",
      "Episode 500\tAverage Score: -128.08\n",
      "Episode 600\tAverage Score: -123.07\n",
      "Episode 700\tAverage Score: -102.05\n",
      "Episode 800\tAverage Score: -100.96\n",
      "Episode 900\tAverage Score: -104.31\n",
      "Episode 1000\tAverage Score: -102.76\n",
      "Episode 100\tAverage Score: -372.54\n",
      "Episode 200\tAverage Score: -170.11\n",
      "Episode 300\tAverage Score: -123.18\n",
      "Episode 400\tAverage Score: -105.59\n",
      "Episode 500\tAverage Score: -102.13\n",
      "Episode 600\tAverage Score: -97.744\n",
      "Episode 700\tAverage Score: -100.90\n",
      "Episode 800\tAverage Score: -103.68\n",
      "Episode 900\tAverage Score: -105.38\n",
      "Episode 1000\tAverage Score: -92.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from config1 import Agent\n",
    "\n",
    "#agent = Agent(state_size=6,action_size = 3,seed = 0)\n",
    "no_siblings = 2\n",
    "sibling_scores = []\n",
    "sibling_lives = np.zeros(no_siblings)\n",
    "\n",
    "\n",
    "def dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                 # list containing scores from each episode\n",
    "    scores_window_printing = deque(maxlen=10) # For printing in the graph\n",
    "    scores_window= deque(maxlen=100)  # last 100 scores for checking if the avg is more than 195\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_window_printing.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))        \n",
    "        if i_episode % 100 == 0: \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=495.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return [np.array(scores),i_episode-100]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "for i in range(no_siblings):\n",
    "    \n",
    "    agent = Agent(state_size=6,action_size = 3,seed = 0)\n",
    "    [temp_scores,sibling_lives[i]] = dqn()\n",
    "    sibling_scores.append(temp_scores)\n",
    "    plt.plot(np.arange(len(temp_scores)), temp_scores)\n",
    "    \n",
    "        \n",
    "    \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-500.  -494.2 -479.5 ...  -75.   -91.   -77.9]\n",
      "-----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temp_life' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e09377ae679c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_life\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msibling_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_life' is not defined"
     ]
    }
   ],
   "source": [
    "print(temp_scores)\n",
    "print('-----')\n",
    "print(temp_life)\n",
    "\n",
    "sibling_scores.append(temp_scores)\n",
    "sibling_lives[i] = temp_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2- (+Q + E - T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config2 import Agent\n",
    "\n",
    "#agent = Agent(state_size=6,action_size = 3,seed = 0)\n",
    "no_siblings = 2\n",
    "sibling_scores = []\n",
    "sibling_lives = np.zeros(no_siblings)\n",
    "\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores2 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores2)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores2\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores2 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores2[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores2 = np.divide(scores2,avg_iter)\n",
    "    \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores2)), scores2)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3- (+Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from config3 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores3 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores3)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores3\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores1 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores3[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores3 = np.divide(scores3,avg_iter)\n",
    "    \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores3)), scores3)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4- (+Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (YES)\n",
    "Frequency of network switch - Every 5 episodes\n",
    "\n",
    "###  Experience Replay (NO)\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config4 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores4 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores4)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores4\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores1 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores4[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores4 = np.divide(scores4,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores4)), scores4)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5- (-Q +E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Networks Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (YES)\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config5 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores5 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores5)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores5\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores5 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores5[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores5 = np.divide(scores5,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores5)), scores5)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: (-Q +E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency (NO)\n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay (YES)\n",
    "Total Replay Buffer Size - 10,000\n",
    "Mini Batch Size - 64\n",
    "\n",
    "### Loss Clipping (NO)\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config6 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "n_players = 10\n",
    "results = np.zeros([])\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores6 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores6)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores6\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores6 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores6[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores6 = np.divide(scores6,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores6)), scores6)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 7: (-Q -E +T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "Gradient is clipped to 1 & -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config7 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores7 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores7)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores7\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores7 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores7[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores7 = np.divide(scores7,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores7)), scores7)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 8: (-Q -E -T)\n",
    "\n",
    "### Neural Network \n",
    "Input Layer - 4 nodes (State Shape)\n",
    "Hidden Layer 1 - 64 nodes\n",
    "Hidden Layer 2 - 64 nodes\n",
    "Output Layer - 2 nodes (Action Space)\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "### Network Update Frequency \n",
    "Frequency of network switch - Every episode\n",
    "\n",
    "###  Experience Replay\n",
    "No Experience Replay / Experience Replay of Size 1\n",
    "\n",
    "### Loss Clipping\n",
    "No Gradient clipping present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config8 import Agent\n",
    "\n",
    "agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            scores.append(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores8 = np.array([0])\n",
    "avg_iter = np.array([0])\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    agent = Agent(state_size=4,action_size = 2,seed = 0)\n",
    "    temp_scores = dqn()\n",
    "    temp_avg_iter = np.ones_like(temp_scores)\n",
    "    \n",
    "    existing_length = len(scores8)\n",
    "    new_length = len(temp_scores)\n",
    "    \n",
    "    if existing_length <= new_length:    \n",
    "        \n",
    "        temp_scores[:existing_length] += scores8\n",
    "        temp_avg_iter[:existing_length] += avg_iter\n",
    "        \n",
    "        scores8 = temp_scores\n",
    "        avg_iter = temp_avg_iter\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        scores8[:new_length] += temp_scores\n",
    "        avg_iter[:new_length] += temp_avg_iter\n",
    "    \n",
    "scores8 = np.divide(scores8,avg_iter)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores8)), scores8)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='case 1')\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='g', label='case 2')\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='r', label='case 3')\n",
    "plt.plot(np.arange(len(scores4)), scores4, color='c', label='case 4')\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='case 5')\n",
    "plt.plot(np.arange(len(scores6)), scores6, color='y', label='case 6')\n",
    "plt.plot(np.arange(len(scores7)), scores7, color='k', label='case 7')\n",
    "plt.plot(np.arange(len(scores8)), scores8, color='w', label='case 8')\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Target Network on')\n",
    "\n",
    "plt.plot(np.arange(len(scores5)), scores5, color='m', label='Target Network off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='EXP Replay on')\n",
    "\n",
    "plt.plot(np.arange(len(scores3)), scores3, color='m', label='EXP Replay off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy to create an array X\n",
    "X = np.arange(0, 2000, 100)\n",
    "\n",
    "plt.plot(np.arange(len(scores1)), scores1, color='b', label='Truncation on')\n",
    "\n",
    "plt.plot(np.arange(len(scores2)), scores2, color='m', label='Truncation off')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episodes x100#\")\n",
    "plt.ylabel(\"Avg Score\")\n",
    "plt.title(\"Variations in DQN\")\n",
    "  \n",
    "# Adding legend, which helps us recognize the curve according to it's color\n",
    "plt.legend()\n",
    "  \n",
    "# To load the display window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
